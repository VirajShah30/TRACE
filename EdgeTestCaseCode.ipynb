{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XLpD_XhohNuK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\msmay\\documents\\github\\trace\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hWPNPAcChTb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\msmay\\Documents\\GitHub\\TRACE\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx7KeswnhpK8"
   },
   "source": [
    "## ***Get Dataset - BigCodeBench***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wgnHDEJFhWqC"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.0_hf[:10]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afjqTCSYh1Hj"
   },
   "source": [
    "## ***Initialize Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g6imT1GhZCz"
   },
   "outputs": [],
   "source": [
    "model_names = {                               #TODO : Change Model to check improvement\n",
    "    \"CodeLlama\": \"codellama/CodeLlama-7b-hf\",\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEU8TPxnheMH"
   },
   "outputs": [],
   "source": [
    "for name, model_id in model_names.items():\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(model_id)\n",
    "    models[name] = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF6dnQdeh7tm"
   },
   "source": [
    "## ***Feedback Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to handle adding test case methods to the test case class\n",
    "# Used in generate_feedback() for updating test cases\n",
    "\n",
    "import ast\n",
    "import astor\n",
    "\n",
    "def test_case_updator(existing_test_code: str, new_test_method: str) -> str:\n",
    "    \"\"\"\n",
    "    Add a new method to existing Python TestClass represented as a string.\n",
    "\n",
    "    Args:\n",
    "        existing_test_code (str): The Python code for the class.\n",
    "        new_test_method (str): The Python code for the method to add.\n",
    "\n",
    "    Returns:\n",
    "        str: The updated Python code.\n",
    "    \"\"\"\n",
    "    # Parsing the existing code and the new method into AST\n",
    "    tree = ast.parse(existing_test_code)\n",
    "    method_node = ast.parse(new_test_method).body[0]\n",
    "    \n",
    "    # Finding the class definition in the AST\n",
    "    for node in tree.body:\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            node.body.append(method_node)\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"No class definition found in the existing code.\")\n",
    "\n",
    "    # Returning the updated code as a string\n",
    "    return astor.to_source(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge Test Case Generation Model Function for generate_feedback()\n",
    "from transformers import T5ForConditionalGeneration, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# Setup CodeT5 model for generate_feedback \n",
    "def setup_codet5():\n",
    "    model_name = \"Salesforce/codet5-small\"\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = setup_codet5()\n",
    "\n",
    "\n",
    "def generated_edge_test_cases(function_code, errors):\n",
    "    \"\"\"\n",
    "    Generate edge case tests using CodeT5\n",
    "    \n",
    "    Args:\n",
    "        function_code (str): The source code of function to test\n",
    "        errors (list): List of error strings, e.g. \n",
    "                      [\"IndexError: list index out of range\",\n",
    "                       \"TypeError: '>' not supported between instances of 'NoneType' and 'int'\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Generate Python unittest test methods for edge cases.\n",
    "Function to test:\n",
    "{function_code}\n",
    "\n",
    "Current test failures:\n",
    "{errors}\n",
    "\n",
    "Requirements:\n",
    "1. Test edge cases and boundary conditions\n",
    "2. Handle error cases\n",
    "3. Follow unittest format\n",
    "4. Use proper indentation (4 spaces)\n",
    "5. Include meaningful assertions\n",
    "\n",
    "Return ONLY the test methods, without the class definition.\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5erEP-eQi5sN"
   },
   "outputs": [],
   "source": [
    "def generate_feedback(evaluation_result, g_code, tc_code):\n",
    "    \"\"\"\n",
    "    Generate feedback and edge test cases based on evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_result (dict): Contains pass_ratio, executability, and errors\n",
    "        g_code (str): Generated code from the model\n",
    "        tc_code (str): Current test cases code\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prompt for next iteration, updated test code)\n",
    "    \"\"\"\n",
    "    # Implement function for providing feedback to the model (solely based on evaluation results)\n",
    "\n",
    "    # Evaluation results - pass_ratio, executability, errors\n",
    "    pass_ratio = evaluation_result.get('pass_ratio', 0)\n",
    "    executability = evaluation_result.get('executability', False)\n",
    "    errors = evaluation_result.get('errors', [])\n",
    "\n",
    "    # Executability = True, Pass Ratio < 1, Errors > 0  -> Generate edge test cases\n",
    "    if executability:\n",
    "        # Generating edge test cases if errors are present and/or pass ratio is less than 1\n",
    "        if pass_ratio < 1 or len(errors) > 0:\n",
    "            # Generate edge test cases\n",
    "            new_tests = generated_edge_test_cases(g_code, errors)\n",
    "            prompt = f'Code is executable. However, the pass ratio is less than 1. Please check the following errors - {errors}.'\n",
    "            # Append to dataset['test'] (using another function)\n",
    "            updated_test_code = test_case_updator(tc_code, new_tests)\n",
    "    # Executability = False -> Update the prompt and ask the model to generate the code again [skip test case update]\n",
    "    else:\n",
    "        updated_test_code = tc_code\n",
    "        prompt = f'Code not executable. These are the following errors - {errors}. Please regenerate the code.'\n",
    "\n",
    "    # Returning prompt and generated test cases code (including any previous test cases)\n",
    "    return prompt, updated_test_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_feedback(evaluation_result, g_code, tc_code):\n",
    "#     \"\"\"\n",
    "#     Generate feedback and edge cases based on evaluation results.\n",
    "#     Args:\n",
    "#         evaluation_result (dict): Contains pass_ratio, executability, and errors\n",
    "#         g_code (str): Generated code\n",
    "#         tc_code (str): Current test cases\n",
    "#     Returns:\n",
    "#         tuple: (prompt for next iteration, updated test code)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     pass_ratio = evaluation_result.get('pass_ratio', 0)\n",
    "#     executability = evaluation_result.get('executability', False)\n",
    "#     errors = evaluation_result.get('errors', [])\n",
    "\n",
    "#     if executability and pass_ratio < 1 and len(errors) > 0:\n",
    "#         # Case III: Generate edge cases\n",
    "#         try:\n",
    "#             # Generate new edge case tests\n",
    "#             new_tests = generate_edge_case_prompt(g_code, errors)\n",
    "            \n",
    "#             # Create feedback prompt (Or to Asks for analysis of failure causes and give solution, One more LLM responds)\n",
    "#             prompt = f\"\"\"\n",
    "# Fix the implementation to handle these edge cases:\n",
    "\n",
    "# Current code:\n",
    "# {g_code}\n",
    "\n",
    "# Test failures:\n",
    "# {format_errors(errors)}\n",
    "\n",
    "\n",
    "# Based on the test failures:\n",
    "# 1. Analyze what edge cases are causing failures\n",
    "# 2. Consider input validation needed\n",
    "# 3. Handle potential error conditions\n",
    "# 4. Implement proper error handling\n",
    "# 5. Add necessary boundary checks\n",
    "\n",
    "# Provide an improved implementation that addresses these issues.\n",
    "# \"\"\"\n",
    "#             # Update test code with new edge cases\n",
    "#             updated_test_code = append_test_cases(tc_code, new_tests)\n",
    "#             return prompt, updated_test_code\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error generating Case III: Generate edge cases: {e}\")\n",
    "#             return \"\", tc_code\n",
    "\n",
    "#     elif not executability:\n",
    "#         # Case IV & V: Focus on fixing execution errors\n",
    "#         prompt = f\"\"\"\n",
    "# Fix these execution errors:\n",
    "\n",
    "# Code:\n",
    "# {g_code}\n",
    "\n",
    "# Errors:\n",
    "# {format_errors(errors)}\n",
    "\n",
    "# Provide a corrected implementation.\n",
    "# \"\"\"\n",
    "#         return prompt, tc_code\n",
    "\n",
    "#     return \"\", tc_code\n",
    "\n",
    "# def format_errors(errors):\n",
    "#     \"\"\"Format error messages for prompt\"\"\"\n",
    "#     return \"\\n\".join(f\"- {err.split('\\n')[0]}\" for err in errors)\n",
    "\n",
    "# def append_test_cases(existing_tests, new_tests):\n",
    "#     \"\"\"Append new tests while maintaining class structure\"\"\"\n",
    "#     try:\n",
    "#         module = cst.parse_module(existing_tests)\n",
    "        \n",
    "#         class TestAppender(cst.CSTTransformer):\n",
    "#             def __init__(self, new_test_code):\n",
    "#                 self.new_tests = cst.parse_module(new_test_code).body\n",
    "                \n",
    "#             def leave_ClassDef(self, original_node, updated_node):\n",
    "#                 if \"TestClass\" in original_node.name.value:\n",
    "#                     return updated_node.with_changes(\n",
    "#                         body=updated_node.body + self.new_tests\n",
    "#                     )\n",
    "#                 return updated_node\n",
    "\n",
    "#         transformer = TestAppender(new_tests)\n",
    "#         modified = module.visit(transformer)\n",
    "#         return modified.code\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error appending tests: {e}\")\n",
    "#         return existing_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmGdchNsiEJi"
   },
   "outputs": [],
   "source": [
    "# This function has been updated to return the prompt and test code for the next iteration(s)\n",
    "# [IMPORTANT : Run evaluation block before running this cell]\n",
    "def generate_score(prompt, model, tokenizer, data, max_length=1000):\n",
    "    n = 5 #TODO : Set N based on your inplementation\n",
    "    test_code = data['test'] # Initially setting from BigCodeBench, will be updated in the loop\n",
    "    for _ in range(n):\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "      output = model.generate(**inputs, max_length=max_length, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "      evaluation_result = evaluate_code_generation(output, test_code, data['libs'])\n",
    "      prompt, test_code = generate_feedback(evaluation_result, output, test_code)\n",
    "    return evaluation_result['pass_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset['test']) # Tests already present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset['libs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['doc_struct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTwybgwGiOFq"
   },
   "outputs": [],
   "source": [
    "generated_scores = {name: [] for name in model_names}\n",
    "\n",
    "for example in dataset:\n",
    "    prompt = example[\"instruct_prompt\"]\n",
    "    print(\"Prompt:\" + prompt)\n",
    "    for model_name in model_names:\n",
    "        generated_score = generate_score(prompt, models[model_name], tokenizers[model_name], example)\n",
    "        generated_scores[model_name].append(generated_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDdApHiZiS3_"
   },
   "source": [
    "## ***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldtbV07YiViU"
   },
   "outputs": [],
   "source": [
    "# # Evaluation Setup with BLEU (or CodeBLEU if available)\n",
    "# bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# # Prepare reference code for evaluation\n",
    "# references = [example[\"canonical_solution\"] for example in dataset]\n",
    "\n",
    "# # Evaluate each model's generated code against the reference code\n",
    "# evaluation_scores = {}\n",
    "# for model_name, codes in generated_codes.items():\n",
    "#     bleu_score = bleu_metric.compute(predictions=codes, references=references)\n",
    "#     evaluation_scores[model_name] = bleu_score[\"bleu\"]\n",
    "#     print(f\"{model_name} BLEU Score:\", bleu_score[\"bleu\"])\n",
    "\n",
    "# # Print final evaluation summary\n",
    "# print(\"\\n=== Evaluation Summary ===\")\n",
    "# for model_name, score in evaluation_scores.items():\n",
    "#     print(f\"{model_name} BLEU Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest.mock import patch\n",
    "\n",
    "def evaluate_code_generation(generated_code, test_cases_code, libraries):\n",
    "    \"\"\"\n",
    "    Dynamically evaluates the generated code based on pass ratio and executability.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_code: A string containing the code to be evaluated.\n",
    "    - test_cases_code: A string containing the test cases to be executed.\n",
    "    - libraries: A list of library names to import and make available in the execution context.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing 'pass_ratio', 'executability', and 'errors'.\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Prepare a local namespace for executing the generated code\n",
    "    local_namespace = {}\n",
    "    error_messages = []\n",
    "\n",
    "    # Import libraries dynamically and add them to the namespace\n",
    "    for lib in libraries:\n",
    "        try:\n",
    "            # Import each library and add it to the local namespace\n",
    "            local_namespace[lib] = __import__(lib)\n",
    "        except ImportError as e:\n",
    "            error_messages.append(f\"Error importing {lib}: {e}\")\n",
    "            return {\n",
    "                'pass_ratio': 0,\n",
    "                'executability': False,\n",
    "                'errors': error_messages\n",
    "            }\n",
    "    \n",
    "    try:\n",
    "        # Execute the generated code in the provided namespace\n",
    "        exec(generated_code, local_namespace, local_namespace)\n",
    "        executability = True\n",
    "    except Exception as e:\n",
    "        error_messages.append(f\"Execution Error: {e}\")\n",
    "        executability = False\n",
    "\n",
    "    if not executability:\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': executability,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Add task_func to local_namespace so it can be accessed by tests\n",
    "    task_func = local_namespace.get('task_func')\n",
    "    \n",
    "    if not task_func:\n",
    "        error_messages.append(\"task_func is not defined in the generated code.\")\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Dynamically create a TestCase class from the provided test cases code\n",
    "    try:\n",
    "        exec(test_cases_code, {'task_func': task_func, 'patch': patch}, local_namespace)\n",
    "    except Exception as e:\n",
    "        error_messages.append(f\"Test case execution error: {e}\")\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Extract the TestCase class from the local namespace\n",
    "    TestClass = local_namespace.get('TestCases')\n",
    "\n",
    "    if not TestClass:\n",
    "        error_messages.append(\"TestCases class not found in provided test cases code.\")\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Define a custom test suite\n",
    "    class CustomTestSuite(unittest.TestSuite):\n",
    "        def run(self, result, debug=False):\n",
    "            super().run(result, debug)\n",
    "            return result\n",
    "\n",
    "    # Run the tests using unittest framework\n",
    "    suite = CustomTestSuite()\n",
    "    suite.addTest(unittest.makeSuite(TestClass))\n",
    "    \n",
    "    runner = unittest.TextTestRunner()\n",
    "    \n",
    "    # Capture results\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    # Calculate pass ratio\n",
    "    pass_ratio = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0\n",
    "    \n",
    "    # Collect errors from test results\n",
    "    for failure in result.failures + result.errors:\n",
    "        error_messages.append(str(failure))\n",
    "\n",
    "    # Clear the local namespace after execution\n",
    "    local_namespace.clear()\n",
    "    \n",
    "    return {\n",
    "        'pass_ratio': pass_ratio,\n",
    "        'executability': len(result.errors) == 0,\n",
    "        'errors': error_messages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msmay\\AppData\\Local\\Temp\\ipykernel_4692\\3258023429.py:92: DeprecationWarning: unittest.makeSuite() is deprecated and will be removed in Python 3.13. Please use unittest.TestLoader.loadTestsFromTestCase() instead.\n",
      "  suite.addTest(unittest.makeSuite(TestClass))\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 7.611s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass_ratio': 1.0, 'executability': True, 'errors': []}\n"
     ]
    }
   ],
   "source": [
    "# Example usage with dynamic inputs and libraries list\n",
    "generated_code = \"\"\"\n",
    "import itertools\n",
    "from random import shuffle\n",
    "\n",
    "def task_func(numbers=list(range(1, 11))):\n",
    "    permutations = list(itertools.permutations(numbers))\n",
    "    sum_diffs = 0\n",
    "\n",
    "    for perm in permutations:\n",
    "        perm = list(perm)\n",
    "        shuffle(perm)\n",
    "        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n",
    "        sum_diffs += sum(diffs)\n",
    "\n",
    "    avg_sum_diffs = sum_diffs / len(permutations) if permutations else 0\n",
    "\n",
    "    return avg_sum_diffs\n",
    "\"\"\"\n",
    "\n",
    "test_cases_code = \"\"\"\n",
    "import unittest\n",
    "\n",
    "class TestCases(unittest.TestCase):\n",
    "    \n",
    "    def test_default_numbers(self):\n",
    "        result = task_func()\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertGreater(result, 0)\n",
    "\n",
    "# Additional tests omitted for brevity...\n",
    "\"\"\"\n",
    "\n",
    "# List of libraries to import and use within exec()\n",
    "libraries_to_import = []\n",
    "\n",
    "# Evaluate the generated code with dynamic inputs and specified libraries\n",
    "evaluation_result = evaluate_code_generation(generated_code, test_cases_code, libraries_to_import)\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Implement a function to average teh scores from each list in generate_score which is sthe final score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
