{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLpD_XhohNuK"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "hWPNPAcChTb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Get Dataset - BigCodeBench***"
      ],
      "metadata": {
        "id": "gx7KeswnhpK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.0_hf[:10]\")"
      ],
      "metadata": {
        "id": "wgnHDEJFhWqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Initialize Models***"
      ],
      "metadata": {
        "id": "afjqTCSYh1Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = {                               #TODO : Change Model to check improvement\n",
        "    \"CodeLlama\": \"codellama/CodeLlama-7b-hf\",\n",
        "}\n",
        "\n",
        "tokenizers = {}\n",
        "models = {}"
      ],
      "metadata": {
        "id": "5g6imT1GhZCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_id in model_names.items():\n",
        "    tokenizers[name] = AutoTokenizer.from_pretrained(model_id)\n",
        "    models[name] = AutoModelForCausalLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "tEU8TPxnheMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Feedback Model***"
      ],
      "metadata": {
        "id": "CF6dnQdeh7tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_feedback():\n",
        "    # TODO : Implement funtion for Reinforcement\n",
        "    pass"
      ],
      "metadata": {
        "id": "5erEP-eQi5sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code(prompt, model, tokenizer, solution, max_length=1000):\n",
        "    n = 5 #TODO : Set N based on your inplementation\n",
        "    for _ in range(n):\n",
        "      inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "      output = model.generate(**inputs, max_length=max_length, temperature=0.7, top_p=0.9, do_sample=True)\n",
        "      bleu_score = bleu_metric.compute(predictions=output, references=solution)\n",
        "      prompt = generate_feedback() #TODO : Implement funtion partametrs for Reinforcement\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "WmGdchNsiEJi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_codes = {name: [] for name in model_names}\n",
        "\n",
        "for example in dataset:\n",
        "    print(\"Prompt:\")\n",
        "    prompt = example[\"complete_prompt\"]\n",
        "    solution = example[\"canonical_solution\"]\n",
        "    for model_name in model_names:\n",
        "        generated_code = generate_code(prompt, models[model_name], tokenizers[model_name], solution)\n",
        "        generated_codes[model_name].append(generated_code)"
      ],
      "metadata": {
        "id": "fTwybgwGiOFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Evaluation***"
      ],
      "metadata": {
        "id": "NDdApHiZiS3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Setup with BLEU (or CodeBLEU if available)\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "\n",
        "# Prepare reference code for evaluation\n",
        "references = [example[\"canonical_solution\"] for example in dataset]\n",
        "\n",
        "# Evaluate each model's generated code against the reference code\n",
        "evaluation_scores = {}\n",
        "for model_name, codes in generated_codes.items():\n",
        "    bleu_score = bleu_metric.compute(predictions=codes, references=references)\n",
        "    evaluation_scores[model_name] = bleu_score[\"bleu\"]\n",
        "    print(f\"{model_name} BLEU Score:\", bleu_score[\"bleu\"])\n",
        "\n",
        "# Print final evaluation summary\n",
        "print(\"\\n=== Evaluation Summary ===\")\n",
        "for model_name, score in evaluation_scores.items():\n",
        "    print(f\"{model_name} BLEU Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "ldtbV07YiViU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}