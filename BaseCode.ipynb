{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLpD_XhohNuK"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWPNPAcChTb1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx7KeswnhpK8"
   },
   "source": [
    "## ***Get Dataset - BigCodeBench***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgnHDEJFhWqC"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.0_hf[:10]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afjqTCSYh1Hj"
   },
   "source": [
    "## ***Initialize Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g6imT1GhZCz"
   },
   "outputs": [],
   "source": [
    "model_names = {                               #TODO : Change Model to check improvement\n",
    "    \"CodeLlama\": \"codellama/CodeLlama-7b-hf\",\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEU8TPxnheMH"
   },
   "outputs": [],
   "source": [
    "for name, model_id in model_names.items():\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(model_id)\n",
    "    models[name] = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF6dnQdeh7tm"
   },
   "source": [
    "## ***Feedback Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5erEP-eQi5sN"
   },
   "outputs": [],
   "source": [
    "def generate_feedback(evaluation_result):\n",
    "    # TODO : Implement funtion for Reinforcement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmGdchNsiEJi"
   },
   "outputs": [],
   "source": [
    "#[IMPORTANT : Run evaluation block before running this cell]\n",
    "def generate_score(prompt, model, tokenizer, data, max_length=1000):\n",
    "    n = 5 #TODO : Set N based on your inplementation\n",
    "    for _ in range(n):\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "      output = model.generate(**inputs, max_length=max_length, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "      evaluation_result = evaluate_code_generation(output, data['test'], data['libs'])\n",
    "      prompt = generate_feedback(evaluation_result) #TODO : Implement funtion partametrs for Reinforcement\n",
    "    return evaluation_result['pass_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTwybgwGiOFq"
   },
   "outputs": [],
   "source": [
    "generated_scores = {name: [] for name in model_names}\n",
    "\n",
    "for example in dataset:\n",
    "    prompt = example[\"instruct_prompt\"]\n",
    "    print(\"Prompt:\" + prompt)\n",
    "    for model_name in model_names:\n",
    "        generated_score = generate_score(prompt, models[model_name], tokenizers[model_name], example)\n",
    "        generated_scores[model_name].append(generated_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDdApHiZiS3_"
   },
   "source": [
    "## ***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldtbV07YiViU"
   },
   "outputs": [],
   "source": [
    "# # Evaluation Setup with BLEU (or CodeBLEU if available)\n",
    "# bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# # Prepare reference code for evaluation\n",
    "# references = [example[\"canonical_solution\"] for example in dataset]\n",
    "\n",
    "# # Evaluate each model's generated code against the reference code\n",
    "# evaluation_scores = {}\n",
    "# for model_name, codes in generated_codes.items():\n",
    "#     bleu_score = bleu_metric.compute(predictions=codes, references=references)\n",
    "#     evaluation_scores[model_name] = bleu_score[\"bleu\"]\n",
    "#     print(f\"{model_name} BLEU Score:\", bleu_score[\"bleu\"])\n",
    "\n",
    "# # Print final evaluation summary\n",
    "# print(\"\\n=== Evaluation Summary ===\")\n",
    "# for model_name, score in evaluation_scores.items():\n",
    "#     print(f\"{model_name} BLEU Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest.mock import patch\n",
    "\n",
    "def evaluate_code_generation(generated_code, test_cases_code, libraries):\n",
    "    \"\"\"\n",
    "    Dynamically evaluates the generated code based on pass ratio and executability.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_code: A string containing the code to be evaluated.\n",
    "    - test_cases_code: A string containing the test cases to be executed.\n",
    "    - libraries: A list of library names to import and make available in the execution context.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing 'pass_ratio', 'executability', and 'errors'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare a local namespace for executing the generated code\n",
    "    local_namespace = {}\n",
    "    error_messages = []\n",
    "\n",
    "    # Import libraries dynamically and add them to the namespace\n",
    "    for lib in libraries:\n",
    "        try:\n",
    "            # Import each library and add it to the local namespace\n",
    "            local_namespace[lib] = __import__(lib)\n",
    "        except ImportError as e:\n",
    "            error_messages.append(f\"Error importing {lib}: {e}\")\n",
    "            return {\n",
    "                'pass_ratio': 0,\n",
    "                'executability': False,\n",
    "                'errors': error_messages\n",
    "            }\n",
    "    \n",
    "    try:\n",
    "        # Execute the generated code in the provided namespace\n",
    "        exec(generated_code, local_namespace, local_namespace)\n",
    "        executability = True\n",
    "    except Exception as e:\n",
    "        error_messages.append(f\"Execution Error: {e}\")\n",
    "        executability = False\n",
    "\n",
    "    if not executability:\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': executability,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Add task_func to local_namespace so it can be accessed by tests\n",
    "    task_func = local_namespace.get('task_func')\n",
    "    \n",
    "    if not task_func:\n",
    "        error_messages.append(\"task_func is not defined in the generated code.\")\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Dynamically create a TestCase class from the provided test cases code\n",
    "    try:\n",
    "        exec(test_cases_code, {'task_func': task_func, 'patch': patch}, local_namespace)\n",
    "    except Exception as e:\n",
    "        error_messages.append(f\"Test case execution error: {e}\")\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Extract the TestCase class from the local namespace\n",
    "    TestClass = local_namespace.get('TestCases')\n",
    "\n",
    "    if not TestClass:\n",
    "        error_messages.append(\"TestCases class not found in provided test cases code.\")\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': error_messages\n",
    "        }\n",
    "\n",
    "    # Define a custom test suite\n",
    "    class CustomTestSuite(unittest.TestSuite):\n",
    "        def run(self, result, debug=False):\n",
    "            super().run(result, debug)\n",
    "            return result\n",
    "\n",
    "    # Run the tests using unittest framework\n",
    "    suite = CustomTestSuite()\n",
    "    suite.addTest(unittest.makeSuite(TestClass))\n",
    "    \n",
    "    runner = unittest.TextTestRunner()\n",
    "    \n",
    "    # Capture results\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    # Calculate pass ratio\n",
    "    pass_ratio = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0\n",
    "    \n",
    "    # Collect errors from test results\n",
    "    for failure in result.failures + result.errors:\n",
    "        error_messages.append(str(failure))\n",
    "\n",
    "    # Clear the local namespace after execution\n",
    "    local_namespace.clear()\n",
    "    \n",
    "    return {\n",
    "        'pass_ratio': pass_ratio,\n",
    "        'executability': len(result.errors) == 0,\n",
    "        'errors': error_messages\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with dynamic inputs and libraries list\n",
    "generated_code = \"\"\"\n",
    "import itertools\n",
    "from random import shuffle\n",
    "\n",
    "def task_func(numbers=list(range(1, 11))):\n",
    "    permutations = list(itertools.permutations(numbers))\n",
    "    sum_diffs = 0\n",
    "\n",
    "    for perm in permutations:\n",
    "        perm = list(perm)\n",
    "        shuffle(perm)\n",
    "        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n",
    "        sum_diffs += sum(diffs)\n",
    "\n",
    "    avg_sum_diffs = sum_diffs / len(permutations) if permutations else 0\n",
    "\n",
    "    return avg_sum_diffs\n",
    "\"\"\"\n",
    "\n",
    "test_cases_code = \"\"\"\n",
    "import unittest\n",
    "\n",
    "class TestCases(unittest.TestCase):\n",
    "    \n",
    "    def test_default_numbers(self):\n",
    "        result = task_func()\n",
    "        self.assertIsInstance(result, float)\n",
    "        self.assertGreater(result, 0)\n",
    "\n",
    "# Additional tests omitted for brevity...\n",
    "\"\"\"\n",
    "\n",
    "# List of libraries to import and use within exec()\n",
    "libraries_to_import = []\n",
    "\n",
    "# Evaluate the generated code with dynamic inputs and specified libraries\n",
    "evaluation_result = evaluate_code_generation(generated_code, test_cases_code, libraries_to_import)\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Implement a function to average teh scores from each list in generate_score which is sthe final score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
