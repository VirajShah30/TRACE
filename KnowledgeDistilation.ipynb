{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLpD_XhohNuK"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWPNPAcChTb1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx7KeswnhpK8"
   },
   "source": [
    "## ***Get Dataset - BigCodeBench***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgnHDEJFhWqC"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bigcode/bigcodebench\", split=\"v0.1.0_hf[:10]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afjqTCSYh1Hj"
   },
   "source": [
    "## ***Initialize Models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g6imT1GhZCz"
   },
   "outputs": [],
   "source": [
    "model_names = {                               #TODO : Change Model to check improvement\n",
    "    \"CodeLlama\": \"codellama/CodeLlama-7b-hf\",\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEU8TPxnheMH"
   },
   "outputs": [],
   "source": [
    "for name, model_id in model_names.items():\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(model_id)\n",
    "    models[name] = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF6dnQdeh7tm"
   },
   "source": [
    "## ***Feedback Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5erEP-eQi5sN"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key from the .env file\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_feedback(initial_prompt, generated_code, evaluation_results):\n",
    "    \"\"\"\n",
    "    Generate a refined prompt using GPT-4 based on evaluation feedback.\n",
    "\n",
    "    Args:\n",
    "        initial_prompt (str): The original user prompt.\n",
    "        generated_code (str): The code generated by Code Llama or similar model.\n",
    "        evaluation_results (dict): Feedback from the evaluator, including issue type and suggestions.\n",
    "\n",
    "    Returns:\n",
    "        str: The refined prompt returned by GPT-4.\n",
    "    \"\"\"\n",
    "    # Determine the type of issue and feedback details\n",
    "    issue_type = evaluation_results.get('type', 'General Issue')\n",
    "    feedback_details = evaluation_results.get('feedback', 'No feedback provided.')\n",
    "    \n",
    "    # Map issue types to corresponding tasks\n",
    "    issue_tasks = {\n",
    "        \"Syntax Error\": \"1. Refine the original prompt to avoid reserved keywords or invalid syntax.\\n\"\n",
    "                        \"2. Suggest corrections or modifications to the generated code to resolve syntax issues.\",\n",
    "        \"Logical Error\": \"1. Refine the original prompt to ensure it explicitly includes all necessary requirements.\\n\"\n",
    "                         \"2. Suggest corrections or modifications to the generated code to address logical errors.\",\n",
    "        \"Optimization Issue\": \"1. Refine the original prompt to emphasize optimization and efficiency.\\n\"\n",
    "                              \"2. Suggest corrections or modifications to the generated code to improve algorithm efficiency.\",\n",
    "        \"Test Case Failure\": \"1. Refine the original prompt to include handling of edge cases.\\n\"\n",
    "                             \"2. Suggest corrections or modifications to the generated code to pass all test cases.\"\n",
    "    }\n",
    "\n",
    "    # Default task if issue type is not found\n",
    "    task = issue_tasks.get(issue_type, \"1. Refine the original prompt.\\n2. Suggest improvements to the generated code.\")\n",
    "    \n",
    "    # Construct GPT-4 input message\n",
    "    system_message = \"You are an expert in refining coding prompts and debugging issues.\"\n",
    "    user_message = f\"\"\"\n",
    "    Below is a scenario where the generated code has a {issue_type}. Help me refine the original prompt or suggest improvements.\n",
    "\n",
    "    Original Prompt:\n",
    "    {initial_prompt}\n",
    "\n",
    "    Generated Code:\n",
    "    {generated_code}\n",
    "\n",
    "    Feedback from Evaluator:\n",
    "    {feedback_details}\n",
    "\n",
    "    Task for GPT-4:\n",
    "    {task}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call GPT-4 API\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "        # Extract and return the refined prompt\n",
    "        refined_prompt = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return refined_prompt\n",
    "    except Exception as e:\n",
    "        return f\"Error during GPT-4 processing: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmGdchNsiEJi"
   },
   "outputs": [],
   "source": [
    "#[IMPORTANT : Run evaluation block before running this cell]\n",
    "def generate_score(prompt, model, tokenizer, data, max_length=1000):\n",
    "    n = 5 #TODO : Set N based on your inplementation\n",
    "    for _ in range(n):\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "      output = model.generate(**inputs, max_length=max_length, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "      evaluation_result = evaluate_code_generation(output, data['test'], data['libs'])\n",
    "      prompt = generate_feedback(evaluation_result) #TODO : Implement funtion partametrs for Reinforcement\n",
    "    return evaluation_result['pass_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTwybgwGiOFq"
   },
   "outputs": [],
   "source": [
    "generated_scores = {name: [] for name in model_names}\n",
    "\n",
    "for example in dataset:\n",
    "    prompt = example[\"instruct_prompt\"]\n",
    "    print(\"Prompt:\" + prompt)\n",
    "    for model_name in model_names:\n",
    "        generated_score = generate_score(prompt, models[model_name], tokenizers[model_name], example)\n",
    "        generated_scores[model_name].append(generated_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDdApHiZiS3_"
   },
   "source": [
    "## ***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldtbV07YiViU"
   },
   "outputs": [],
   "source": [
    "# # Evaluation Setup with BLEU (or CodeBLEU if available)\n",
    "# bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# # Prepare reference code for evaluation\n",
    "# references = [example[\"canonical_solution\"] for example in dataset]\n",
    "\n",
    "# # Evaluate each model's generated code against the reference code\n",
    "# evaluation_scores = {}\n",
    "# for model_name, codes in generated_codes.items():\n",
    "#     bleu_score = bleu_metric.compute(predictions=codes, references=references)\n",
    "#     evaluation_scores[model_name] = bleu_score[\"bleu\"]\n",
    "#     print(f\"{model_name} BLEU Score:\", bleu_score[\"bleu\"])\n",
    "\n",
    "# # Print final evaluation summary\n",
    "# print(\"\\n=== Evaluation Summary ===\")\n",
    "# for model_name, score in evaluation_scores.items():\n",
    "#     print(f\"{model_name} BLEU Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from unittest.mock import patch\n",
    "\n",
    "def evaluate_code_generation(generated_code, test_cases_code, libraries):\n",
    "    \"\"\"\n",
    "    Dynamically evaluates the generated code based on pass ratio and executability.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_code: A string containing the code to be evaluated.\n",
    "    - test_cases_code: A string containing the test cases to be executed.\n",
    "    - libraries: A list of library names to import and make available in the execution context.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing 'pass_ratio', 'executability', and categorized errors.\n",
    "    \"\"\"\n",
    "    # Prepare a local namespace for executing the generated code\n",
    "    local_namespace = {}\n",
    "    errors = {\n",
    "        'Syntax Errors': [],\n",
    "        'Logical Errors': [],\n",
    "        'Optimization Issues': [],\n",
    "        'Test Case Failures': [],\n",
    "    }\n",
    "\n",
    "    # Import libraries dynamically and add them to the namespace\n",
    "    for lib in libraries:\n",
    "        try:\n",
    "            local_namespace[lib] = __import__(lib)\n",
    "        except ImportError as e:\n",
    "            errors['Syntax Errors'].append({'type': 'ImportError', 'message': f\"Error importing {lib}: {e}\"})\n",
    "            return {\n",
    "                'pass_ratio': 0,\n",
    "                'executability': False,\n",
    "                'errors': errors\n",
    "            }\n",
    "\n",
    "    try:\n",
    "        # Execute the generated code in the provided namespace\n",
    "        exec(generated_code, local_namespace, local_namespace)\n",
    "        executability = True\n",
    "    except SyntaxError as e:\n",
    "        errors['Syntax Errors'].append({'type': 'SyntaxError', 'message': str(e)})\n",
    "        executability = False\n",
    "    except Exception as e:\n",
    "        errors['Logical Errors'].append({'type': 'ExecutionError', 'message': str(e)})\n",
    "        executability = False\n",
    "\n",
    "    if not executability:\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': executability,\n",
    "            'errors': errors\n",
    "        }\n",
    "\n",
    "    # Add task_func to local_namespace so it can be accessed by tests\n",
    "    task_func = local_namespace.get('task_func')\n",
    "\n",
    "    if not task_func:\n",
    "        errors['Logical Errors'].append({'type': 'FunctionError', 'message': \"task_func is not defined in the generated code.\"})\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': errors\n",
    "        }\n",
    "\n",
    "    # Dynamically create a TestCase class from the provided test cases code\n",
    "    try:\n",
    "        exec(test_cases_code, {'task_func': task_func, 'patch': patch}, local_namespace)\n",
    "    except SyntaxError as e:\n",
    "        errors['Syntax Errors'].append({'type': 'TestCaseSyntaxError', 'message': str(e)})\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': errors\n",
    "        }\n",
    "    except Exception as e:\n",
    "        errors['Logical Errors'].append({'type': 'TestCaseError', 'message': str(e)})\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': errors\n",
    "        }\n",
    "\n",
    "    # Extract the TestCase class from the local namespace\n",
    "    TestClass = local_namespace.get('TestCases')\n",
    "\n",
    "    if not TestClass:\n",
    "        errors['Logical Errors'].append({'type': 'TestCaseError', 'message': \"TestCases class not found in provided test cases code.\"})\n",
    "        return {\n",
    "            'pass_ratio': 0,\n",
    "            'executability': False,\n",
    "            'errors': errors\n",
    "        }\n",
    "\n",
    "    # Define a custom test suite\n",
    "    class CustomTestSuite(unittest.TestSuite):\n",
    "        def run(self, result, debug=False):\n",
    "            super().run(result, debug)\n",
    "            return result\n",
    "\n",
    "    # Run the tests using unittest framework\n",
    "    suite = CustomTestSuite()\n",
    "    suite.addTest(unittest.makeSuite(TestClass))\n",
    "\n",
    "    runner = unittest.TextTestRunner()\n",
    "\n",
    "    # Capture results\n",
    "    result = runner.run(suite)\n",
    "\n",
    "    # Calculate pass ratio\n",
    "    pass_ratio = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun if result.testsRun > 0 else 0\n",
    "\n",
    "    # Categorize test-related errors\n",
    "    for failure in result.failures:\n",
    "        errors['Test Case Failures'].append({'type': 'TestFailure', 'message': str(failure)})\n",
    "    for error in result.errors:\n",
    "        errors['Test Case Failures'].append({'type': 'TestError', 'message': str(error)})\n",
    "\n",
    "    # Clear the local namespace after execution\n",
    "    local_namespace.clear()\n",
    "\n",
    "    return {\n",
    "        'pass_ratio': pass_ratio,\n",
    "        'executability': len(result.errors) == 0 and len(result.failures) == 0,\n",
    "        'errors': errors\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass_ratio': 0, 'executability': False, 'errors': [{'type': 'TestCaseError', 'message': 'invalid syntax (<string>, line 2)'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example usage with dynamic inputs and libraries list\n",
    "generated_code = \"\"\"\n",
    "import itertools\n",
    "from random import shuffle\n",
    "\n",
    "def task_func(numbers=list(range(1, 11))):\n",
    "    permutations = list(itertools.permutations(numbers))\n",
    "    sum_diffs = 0\n",
    "\n",
    "    for perm in permutations:\n",
    "        perm = list(perm)\n",
    "        shuffle(perm)\n",
    "        diffs = [abs(perm[i] - perm[i+1]) for i in range(len(perm)-1)]\n",
    "        sum_diffs += sum(diffs)\n",
    "\n",
    "    avg_sum_diffs = sum_diffs / len(permutations) if permutations else 0\n",
    "\n",
    "    return avg_sum_diffs\n",
    "\"\"\"\n",
    "\n",
    "test_cases_code = \"\"\"\n",
    "import unittest from unittest.mock import patch from random import seed, shuffle import itertools class TestCases(unittest.TestCase): def test_default_numbers(self): # Test with default number range (1 to 10) to check that the result is a positive float. result = task_func() self.assertIsInstance(result, float) self.assertGreater(result, 0) def test_custom_list(self): # Test with a custom list of small positive integers to ensure proper handling and positive result. result = task_func([1, 2, 3]) self.assertIsInstance(result, float) self.assertGreater(result, 0) def test_negative_numbers(self): # Test with negative numbers to verify the function handles and returns a positive result. result = task_func([-3, -2, -1]) self.assertIsInstance(result, float) self.assertGreater(result, 0) def test_single_element(self): # Test with a single element list to confirm the return is zero since no pairs exist. result = task_func([5]) self.assertIsInstance(result, float) self.assertEqual(result, 0) def test_empty_list(self): # Test with an empty list to ensure the function handles it gracefully and returns zero. result = task_func([]) self.assertIsInstance(result, float) self.assertEqual(result, 0) def test_identical_elements(self): # Test with a list of identical elements to confirm that differences are zero and the average is zero. result = task_func([2, 2, 2]) self.assertIsInstance(result, float) self.assertEqual(result, 0) def test_mixed_numbers(self): # Test with a list of mixed positive and negative numbers to check correct average of differences. result = task_func([-10, 10, -5]) self.assertIsInstance(result, float) self.assertGreater(result, 0) def test_specific_value_with_seed(self): # Set seed for reproducibility and check the computed value with patch('random.shuffle', side_effect=lambda x: seed(42) or shuffle(x)): result = task_func([1, 2, 3]) self.assertAlmostEqual(result, 2.5, delta=0.5) # This expected value should be calculated beforehand def test_large_list_with_seed(self): # Set seed and test with a larger list for specific computed value with patch('random.shuffle', side_effect=lambda x: seed(99) or shuffle(x)): result = task_func(list(range(1, 11))) self.assertAlmostEqual(result, 33.0, delta=0.5) # This expected value should be calculated beforehand def test_random_behavior(self): # Test to ensure different seeds produce different outputs, demonstrating randomness with patch('random.shuffle', side_effect=lambda x: seed(1) or shuffle(x)): result1 = task_func([1, 2, 3]) with patch('random.shuffle', side_effect=lambda x: seed(1) or shuffle(x)): result2 = task_func([1, 2, 4]) self.assertNotEqual(result1, result2)\n",
    "\"\"\"\n",
    "\n",
    "# List of libraries to import and use within exec()\n",
    "libraries_to_import = ['random', 'itertools']\n",
    "\n",
    "# Evaluate the generated code with dynamic inputs and specified libraries\n",
    "evaluation_result = evaluate_code_generation(generated_code, test_cases_code, libraries_to_import)\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Implement a function to average teh scores from each list in generate_score which is sthe final score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
